---
title: "Text Mining"
author: "Alun"
date: "12 March 2016"
output: 
  html_document: 
    toc_depth: 5
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
require(tm)
require(Matrix)
require(cluster)
require(dplyr)
```

# Building Term Document Matrix
First load in the corpus which has been extracted from the html files, collected into documents (books) and cleaned through tokenisation, stop words removal, case folding and class equivalence. Also chapter titles were removed. 

```{r Load Data, echo=FALSE}
corpus <- readRDS("data/corpus.rds")
metadata <- readRDS("data/metadata.rds")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- metadata$shortname
dtm
```

We now have a document term matrix with a lot of sparsity, a very large maximal term length and 24 documents. We can reduce a lot of this sparsity by remove documents which appear in only one document. And look at the most frequent terms. 
```{r Clean terms, echo=FALSE}
dtm2 <- removeSparseTerms(dtm, 0.95) # Remove terms which only appear in 1 document

dtm2_df <- as.data.frame(as.matrix(dtm2))
freq_table <- apply(dtm2_df, 1, function(x) {
    x %>%
        sort(decreasing = T) %>%
        head(10) %>%
        names()
})
rownames(dtm2) <- metadata$shortname
dtm2
```
The sparsity has deceased and the maximum term length is something more reasonable. We can view the most frequent terms of each book to see which terms are highly weighted. 

```{r And look at the high frequency terms, results="asis"}
t(freq_table) %>% 
    xtable::xtable() %>%
    xtable::print.xtable(type = "html")
```
We can see some themes such as the geographic dictionary having terms like river, citi, town and place. However also there are a lot of repeated and low impact terms like now, war, one, two, roman. So we weight the documents with inverse document frequency and normalise so that long books are evenly weighted vs. short books.  

```{r TFIDF, fig.width=10}
dtm3 <- weightTfIdf(dtm2, normalize = T)
dtm3_df <- as.data.frame(as.matrix(dtm3))
dtm3

freq <- sort(colSums(dtm3_df), decreasing = T)
head(freq, 10)

freq_table <- apply(dtm3_df, 1, function(x) {
    x %>%
        sort(decreasing = T) %>%
        head(10) %>%
        names()
})
```
Looking at the frequency table again...
```{r tfIDF table, results="asis"}
t(freq_table) %>% 
    xtable::xtable() %>%
    xtable::print.xtable(type = "html")
```

We can see now using term frequency - inverse document frequency (tf-idf) that the highest weighted terms are now primarily names. herod, justinian etc. This is suggestive that the clustering later will be about the people and places (names) associated with those books. 

Finally we calculate a distance measure. In this case we use cosine similarity. 
```{r Cosine Similarity}
# Convert into sparse matrix from "Matrix" package
m <- Matrix::sparseMatrix(i = dtm3$i, j = dtm3$j, x = dtm3$v,
               dims = c(dtm3$nrow, dtm3$ncol)) 

# Normalise lengths of documents
row_norms <- sqrt(rowSums(m ^ 2))
row_norms <- t(crossprod(sign(m), Diagonal(x = row_norms)))
row_norms@x <- 1/row_norms@x
m_norm <- m * row_norms

# Finally, we can find cosine similarity
sim <- tcrossprod(m_norm)
colnames(sim) <- metadata$shortname
rownames(sim) <- metadata$shortname
dissim <- 1 - sim
```

Plot results
```{r MultiDimensional Scaling, fig.height=10}
plot(agnes(dissim, method = "ward"), which.plots = 2)
heatmap(as.matrix(dissim), Rowv = NULL)
```
After trying many of the different methods and both manhattan and euclidean distance, the clusters never change with the exception of single linkage, where Livy History has its own cluster. 

From the heatmap, where dissimilarity is yellow and similarity is red. You can see the documents are quite dissimilar with few exceptions. 


Spherical k means clustering works on cosine similarity as well. 
```{r spherical k means clustering}
library(skmeans)
clust_sk <- skmeans(dtm3, 8, method='genetic', control = list(verbose=TRUE))
sort(clust_sk$cluster)
summary(silhouette(clust_sk))
```

# High dimensional scaling

```{r CMD}
cmd <- cmdscale(dissim)

plot(cmd, pch = 19, xlim = range(cmd[,1]))
text(cmd, pos = 4, labels = metadata$shortname)
```

# PCA
```{r}
pca <- prcomp(dissim)
plot(pca$rotation[,"PC1"], pca$rotation[,"PC2"])
text(pca$rotation[,"PC1"], pca$rotation[,"PC2"], pos = 4, labels = metadata$shortname)
```

# Analysis
Looking at the cluster of Josephus 2, Josephus 3, Decline and Fall 6 and Peleponesian I. They all reference Herod and Jew in their high weighting list. The description of greece and Pliny natural history mentions arabs and constantinople while livy 5 and josephus 4 which are closely related mentions constantinople and the goths. 


We can see the dictionary isn't appropriately being isolated. This is likely because we got rid of the sparse terms. The thing that made the dictionary so distinct for other text is that it has a large set of terms which are all mentioned only once. 

We can also see that the different set of books are sometimes but not consistently grouped together by any of these methods. An example in the HDS graph is Livy I and Jospehus I being grouped together. 

```{r analysis}
sort(colSums(dtm3_df[ which(metadata$shortname == "Livy I"),]), decreasing = T) %>% head(20)
sort(colSums(dtm3_df[ which(metadata$shortname == "Josphus 1"),]), decreasing = T) %>% head(20)
```



# Self organised maps
Currently doesn't work but here it is working with wine data
```{r, eval=FALSE, include=FALSE}
data(wines)
set.seed(7)

training <- sample(nrow(wines), 120)
Xtraining <- scale(wines[training, ])
Xtest <- scale(wines[-training, ],
               center = attr(Xtraining, "scaled:center"),
               scale = attr(Xtraining, "scaled:scale"))

som.wines <- som(Xtraining, grid = somgrid(5, 5, "hexagonal"))

som.prediction <- predict(som.wines, newdata = Xtest,
                          trainX = Xtraining,
                          trainY = factor(wine.classes[training]))
table(wine.classes[-training], som.prediction$prediction)

```

# Ngrams
```{r Trigrams, eval=FALSE, include=FALSE}
require(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigrams <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
trigrams <- removeSparseTerms(tdm, 0.95)
```
We find with trigrams many of the highly weighted trigrams come from printing constructs such as "na na book" or "vol ii p". To counter this we remove letters, roman numerals and 'na' from our corpus. On the positive side less of these are based on names. 
